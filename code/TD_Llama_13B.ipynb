{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thought Classification using Llama"
      ],
      "metadata": {
        "id": "OSKCV1tOuBG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXKWoOQfcNyW",
        "outputId": "aa9905e7-9af9-4f21-9ee8-46682cc96eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.109.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.36.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37DETDRcUS2",
        "outputId": "74950d66-d6c4-4e4d-9225-70c5f2c4ba0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBJ24Z_gc3av",
        "outputId": "f7305ea8-c155-4a06-940f-89b922fa5cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S8REpa7cGBG",
        "outputId": "6c9a2433-2828-4315-e973-119289509be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.27.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70EebN52dK1r",
        "outputId": "b61f99d5-2510-4df1-e151-e82307f2f710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.48)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.4)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTDYGe0HdRHJ",
        "outputId": "1897bc8e-64d1-44b2-e914-9e62b5e0eb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65u0gJZddaKP",
        "outputId": "009f912f-c4e7-4d80-eac2-03bbe3c719b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zu5ZH9Be17D",
        "outputId": "d74bb6ca-6255-45fb-a1db-0f7f4d33aa48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.44.tar.gz (36.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting scikit-build-core[pyproject]>=0.5.1\n",
            "    Using cached scikit_build_core-0.8.1-py3-none-any.whl (139 kB)\n",
            "  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
            "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
            "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.8.1 tomli-2.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command pip subprocess to install backend dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting ninja>=1.5\n",
            "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "  Collecting cmake>=3.21\n",
            "    Using cached cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "  Installing collected packages: ninja, cmake\n",
            "    Creating /tmp/pip-build-env-ja0zl1my/normal/local/bin\n",
            "    changing mode of /tmp/pip-build-env-ja0zl1my/normal/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-ja0zl1my/normal/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-ja0zl1my/normal/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-ja0zl1my/normal/local/bin/ctest to 755\n",
            "  Successfully installed cmake-3.28.3 ninja-1.11.1.1\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.8.1 using CMake 3.28.3 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m318.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m251.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m294.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.8.1 using CMake 3.28.3 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmphzlvlu1t/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CUDA host compiler is GNU 11.4.0\n",
            "\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:21 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:30 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (2.6s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmphzlvlu1t/build\n",
            "  *** Building project with Ninja...\n",
            "  Change Dir: '/tmp/tmphzlvlu1t/build'\n",
            "\n",
            "  Run Build Command(s): /tmp/pip-build-env-ja0zl1my/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n",
            "  [1/23] cd /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp && /tmp/pip-build-env-ja0zl1my/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  [2/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/build-info.cpp\n",
            "  [3/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-alloc.c\n",
            "  [4/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-backend.c\n",
            "  [5/23] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [6/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/console.cpp\n",
            "  [7/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/sampling.cpp\n",
            "  [8/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [9/23] /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [10/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/train.cpp\n",
            "  [11/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-quants.c\n",
            "  [12/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/common/common.cpp\n",
            "  [13/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml.c\n",
            "  [14/23] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  [15/23] : && /tmp/pip-build-env-ja0zl1my/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
            "  [16/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/llama.cpp\n",
            "  [17/23] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -use_fast_math -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-cuda.cu(645): warning #177-D: function \"warp_reduce_sum(half2)\" was declared but never referenced\n",
            "                                     half2 warp_reduce_sum(half2 a) {\n",
            "                                           ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-cuda.cu(666): warning #177-D: function \"warp_reduce_max(half2)\" was declared but never referenced\n",
            "                                     half2 warp_reduce_max(half2 x) {\n",
            "                                           ^\n",
            "\n",
            "  /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/vendor/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable \"ksigns64\" was declared but never referenced\n",
            "                           uint64_t ksigns64[128] = {\n",
            "                                    ^\n",
            "\n",
            "  [18/23] : && /tmp/pip-build-env-ja0zl1my/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
            "  [19/23] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
            "  [20/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
            "  [21/23] : && /tmp/pip-build-env-ja0zl1my/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
            "  [22/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  -Wl,-rpath,/tmp/tmphzlvlu1t/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
            "  [23/23] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmphzlvlu1t/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/include/ggml-alloc.h\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/include/ggml-backend.h\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmphzlvlu1t/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmphzlvlu1t/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/lib/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmphzlvlu1t/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/bin/llava-cli\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmphzlvlu1t/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmphzlvlu1t/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmphzlvlu1t/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-yekwepom/llama-cpp-python_2d494cc7f9ba4e4eb0ec93b4793e0329/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.44-cp310-cp310-manylinux_2_35_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.44-cp310-cp310-manylinux_2_35_x86_64.whl size=20493104 sha256=2c6a96fae8c19604ca3b03d92dc621bfcf8488775c27021b5a08d95943615e32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-op_etfnp/wheels/6e/f0/52/1716aa7fefc7eb2a9b76775b0a61fc131b7dcc961e310a048a\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.9.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.26.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/Jinja2-3.1.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.44\n",
            "    Uninstalling llama_cpp_python-0.2.44:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/__pycache__/convert.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/convert-lora-to-ggml.py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/convert.py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/llava-cli\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/include/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/lib/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.2.44.dist-info/\n",
            "      Successfully uninstalled llama_cpp_python-0.2.44\n",
            "Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.44 numpy-1.26.4 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face Hub**"
      ],
      "metadata": {
        "id": "uI4GSBRIgesx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading the models from HF Hub\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yse514l3gji7",
        "outputId": "8f17e8bc-8c81-4bb2-f5d3-1bdbbf8752c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the Llama 2 model**"
      ],
      "metadata": {
        "id": "mMQcsz15gwKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the hf_hub_download function from the Hugging Face Hub library\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Import the Llama class from the llama_cpp library\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "o1-vEKDPgwrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\" # the model is in gguf format"
      ],
      "metadata": {
        "id": "-o4roJ3Gg3h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioIs6MSlhB8T",
        "outputId": "7e75c2cf-4aac-49b9-f7ab-ab42dc18502c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing an instance of the Llama class with specified parameters**"
      ],
      "metadata": {
        "id": "rEGosjfEho5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lcpp_llm = Llama(\n",
        "      model_path=model_path,\n",
        "      n_threads=2, #CPU cores\n",
        "      n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "      n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "      n_ctx=4096, #context window\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjnBJJEygvUq",
        "outputId": "e8203d8b-01e7-4bc5-8a4c-c13fb68b873f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q5_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 41/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   107.42 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  8694.21 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  3200.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    19.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   368.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    10.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqthKe0toYcj",
        "outputId": "ce423649-763d-4725-82cb-32d69771a82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgiluuhkokRD",
        "outputId": "1613c991-3e42-41b6-c326-5f5548e8bc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: setuptools 67.7.2\n",
            "Uninstalling setuptools-67.7.2:\n",
            "  Successfully uninstalled setuptools-67.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "8tVATJoWoqXo",
        "outputId": "27e05402-5a6a-45da-907d-5215d46caa4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setuptools\n",
            "  Downloading setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/819.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/819.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires numpy>=1.20.0, which is not installed.\n",
            "cufflinks 0.17.3 requires numpy>=1.9.2, which is not installed.\n",
            "cvxpy 1.3.3 requires numpy>=1.15, which is not installed.\n",
            "datascience 0.17.6 requires numpy, which is not installed.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires numpy, which is not installed.\n",
            "mlxtend 0.22.0 requires numpy>=1.16.2, which is not installed.\n",
            "moviepy 1.0.3 requires numpy; python_version >= \"2.7\", which is not installed.\n",
            "moviepy 1.0.3 requires numpy>=1.17.3; python_version != \"2.7\", which is not installed.\n",
            "nibabel 4.0.2 requires numpy>=1.17, which is not installed.\n",
            "pandas-gbq 0.19.2 requires numpy>=1.16.6, which is not installed.\n",
            "pymc 5.7.2 requires numpy>=1.15.0, which is not installed.\n",
            "pytensor 2.14.2 requires numpy>=1.17.0, which is not installed.\n",
            "spacy 3.7.2 requires numpy>=1.19.0; python_version >= \"3.9\", which is not installed.\n",
            "tensorboard 2.15.2 requires numpy>=1.12.0, which is not installed.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, which is not installed.\n",
            "thinc 8.2.3 requires numpy>=1.19.0; python_version >= \"3.9\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-69.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGmQ5B3cnC9d",
        "outputId": "c7fe1fea-58b6-4180-af97-5d1a29385f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Dataset**"
      ],
      "metadata": {
        "id": "fT_QRCP9L8pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3m00pDsOJfn",
        "outputId": "bd17e9a0-d838-41cd-e948-34f2c28152e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the random module for generating random numbers or choices.\n",
        "import random\n",
        "\n",
        "# Import the pandas library and alias it as 'pd' for easier use.\n",
        "import pandas as pd\n",
        "\n",
        "#for split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import the 'json' module for handling JSON data.\n",
        "import json\n",
        "\n",
        "# Import the 'numpy' library and alias it as 'np' for convenience.\n",
        "import numpy as np\n",
        "\n",
        "# Import the 'Counter' class from the 'collections' module for counting occurrences of elements.\n",
        "from collections import Counter\n",
        "\n",
        "# Import 'tqdm' for displaying progress bars when iterating over data.\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "vdIbSmqhhTsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/data_thought.csv')"
      ],
      "metadata": {
        "id": "WOtRutGfMGFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the shape of the DataFrame\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft2uISYsPZ8v",
        "outputId": "4e74f99c-afa8-48f6-ef6b-f8eee453595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156785, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop duplicate thoughts\n",
        "data.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "PczWyLtGPmIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reseting the index of data frame since some rows have been deleted\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "U6OW91umQgIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(how='any',axis=0) #drop null values"
      ],
      "metadata": {
        "id": "DZ5s8ZtIQnT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER4UsiaJQ6u5",
        "outputId": "43394212-fe00-4fb9-a3a9-6369fe6c19ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(129080, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAI6NdjSRCgO",
        "outputId": "617004de-c0b5-4e11-a0a9-0a90bb517794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive    71185\n",
            "negative    57895\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting target variable to numeric labels\n",
        "data.Label = [ 1 if each == \"positive\" else 0 for each in data.Label]"
      ],
      "metadata": {
        "id": "n9iyvO4GRNwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSFeEKcPRYpc",
        "outputId": "907bcbe4-a8ff-4fc8-8349-7e1338efce43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    71185\n",
            "0    57895\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train,x_test,y_train,y_test= train_test_split(data['Thought'],data['Label'], test_size=0.2, random_state=42)\n",
        "train, test= train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "s8bljvuvRbrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXjx2xOAR4aL",
        "outputId": "d0963cf0-59c4-4b01-b3be-ce3e66800139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((103264, 2), (25816, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_thought=train.loc[train.Label==1, :].sample(3)\n",
        "negative_thought=train.loc[train.Label==0, :].sample(3)"
      ],
      "metadata": {
        "id": "mYXNG0ySSzrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "positive_examples containing three random positive sentiment examples, and\n",
        "\n",
        "negative_examples containing three random negative sentiment examples."
      ],
      "metadata": {
        "id": "R5pnAmSWXW6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_thought"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "oe0jf2qsT3ky",
        "outputId": "61438dfe-7d0c-4968-9a1b-2892e2cdc504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Thought  Label\n",
              "49150                      Just a pic of my adorable cat      1\n",
              "14590  Ideal chair seat height to kitchen counter rat...      1\n",
              "66721                  Today is going to be a great day!      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-312a5884-2f66-49f0-a746-bd3e5e2cc28b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Thought</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49150</th>\n",
              "      <td>Just a pic of my adorable cat</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14590</th>\n",
              "      <td>Ideal chair seat height to kitchen counter rat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66721</th>\n",
              "      <td>Today is going to be a great day!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-312a5884-2f66-49f0-a746-bd3e5e2cc28b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-312a5884-2f66-49f0-a746-bd3e5e2cc28b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-312a5884-2f66-49f0-a746-bd3e5e2cc28b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21eb0c4d-aaf0-4465-a7aa-97d57726b319\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21eb0c4d-aaf0-4465-a7aa-97d57726b319')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21eb0c4d-aaf0-4465-a7aa-97d57726b319 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "positive_thought",
              "summary": "{\n  \"name\": \"positive_thought\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Thought\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Just a pic of my adorable cat\",\n          \"Ideal chair seat height to kitchen counter ratio for doing dishes lol\",\n          \"Today is going to be a great day!\"\n        ],\n        \"num_unique_values\": 3,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"samples\": [\n          1\n        ],\n        \"num_unique_values\": 1,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_thought"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_PENwmrnVdJR",
        "outputId": "a8374fac-6ded-427e-e3eb-3419d571687f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Thought  Label\n",
              "77622  So I bought the repress of if you leave on the...      0\n",
              "41399  Food sanitation company accused of employing a...      0\n",
              "85162  Poll: Americans support quick diplomatic end t...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c742c20f-5aea-4704-afdb-72322614bf72\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Thought</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>77622</th>\n",
              "      <td>So I bought the repress of if you leave on the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41399</th>\n",
              "      <td>Food sanitation company accused of employing a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85162</th>\n",
              "      <td>Poll: Americans support quick diplomatic end t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c742c20f-5aea-4704-afdb-72322614bf72')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c742c20f-5aea-4704-afdb-72322614bf72 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c742c20f-5aea-4704-afdb-72322614bf72');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4049cdf8-02e0-41b2-8dce-27d9f4ba71ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4049cdf8-02e0-41b2-8dce-27d9f4ba71ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4049cdf8-02e0-41b2-8dce-27d9f4ba71ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "negative_thought",
              "summary": "{\n  \"name\": \"negative_thought\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Thought\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"So I bought the repress of if you leave on the Spotify store thing anyone know when those go out?\",\n          \"Food sanitation company accused of employing at least 31 children on graveyard shifts in slaughterhouses:Packers Sanitation Services, Inc., or PSSI, a company contracted to work at slaughterhouses & meatpacking facilities.. allegedly employed at least 31 kids \\u00e2\\u20ac\\u201d one as young as 13\",\n          \"Poll: Americans support quick diplomatic end to war in Ukraine\"\n        ],\n        \"num_unique_values\": 3,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"samples\": [\n          0\n        ],\n        \"num_unique_values\": 1,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenating the subsets of positive and negative sentiment examples (positive_examples and negative_examples)\n",
        "examples = pd.concat([positive_thought, negative_thought]).to_json(orient='records')\n",
        "\n",
        "#After concatenating the DataFrames, the to_json() method is applied to convert the combined DataFrame into a JSON (JavaScript Object Notation) format.\n",
        "#The orient='records' argument specifies the format in which the JSON data should be structured."
      ],
      "metadata": {
        "id": "B44xNJAlVhde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "5lORszaEyAG0",
        "outputId": "643d7a5d-89d3-48cf-8ebf-9865688adc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[{\"Thought\":\"Just a pic of my adorable cat\",\"Label\":1},{\"Thought\":\"Ideal chair seat height to kitchen counter ratio for doing dishes lol\",\"Label\":1},{\"Thought\":\"Today is going to be a great day!\",\"Label\":1},{\"Thought\":\"So I bought the repress of if you leave on the Spotify store thing anyone know when those go out?\",\"Label\":0},{\"Thought\":\"Food sanitation company accused of employing at least 31 children on graveyard shifts in slaughterhouses:Packers Sanitation Services, Inc., or PSSI, a company contracted to work at slaughterhouses & meatpacking facilities.. allegedly employed at least 31 kids \\\\u00e2\\\\u20ac\\\\u201d one as young as 13\",\"Label\":0},{\"Thought\":\"Poll: Americans support quick diplomatic end to war in Ukraine\",\"Label\":0}]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **System Message**"
      ],
      "metadata": {
        "id": "TxCKTTjEYehN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"[INST]<<SYS>>Classify the human thinking based on the input into the following label\n",
        "-negative\n",
        "-positive\n",
        "\n",
        "Thought will be delimited by triple backticks in the input.\n",
        "Answer only 'positive' or 'negative'.\n",
        "\n",
        "Instructions:\n",
        "1. Classified the input only as: 'positive', 'negative'.\n",
        "\n",
        "Your answer should strictly contain the label: 'positive' or 'negative'.\n",
        "The output should be in JSON format like the following\n",
        "{'Label': <positive, negative>}\n",
        "\n",
        "\n",
        "Some examples of expected output are provided below as guidance.<</SYS>>[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4D1qfvRXXynt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prompt Template**\n",
        "A prompt template is a text string that can take in a set of parameters from the end user and generate a prompt accordingly."
      ],
      "metadata": {
        "id": "5Km_OQdyZavs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template=\"\"\"\n",
        "[INST]```{input_data}```[/INST]\n",
        "{output}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WOF0k6XkZV4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize an empty string to store few-shot examples\n",
        "few_shot_examples=''"
      ],
      "metadata": {
        "id": "5HYwbv-AZ02M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Few-shot Learning**"
      ],
      "metadata": {
        "id": "wB4_BtyKaVkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Iterate through each example in the JSON data which was created earlier\n",
        "for example in json.loads(examples):\n",
        "\n",
        "  #Extract the input data (thought) from the example, excluding the 'Label'\n",
        "  example_input = {i:example[i] for i in example if i!='Label'}\n",
        "\n",
        "  # Determine the thought prediction based on the 'Label' value\n",
        "  if example['Label'] == 0:\n",
        "    example_prediction ='negative'\n",
        "  else:\n",
        "    example_prediction ='positive'\n",
        "\n",
        "  # Concatenate the input data and the predicted thought\n",
        "  # using a template and add it to the 'few_shot_examples' string\n",
        "\n",
        "  few_shot_examples += prompt_template.format(\n",
        "      input_data=example_input['Thought'],    ###input_data is used in the prompt_template\n",
        "      output=example_prediction               ###output is used in the prompt_template\n",
        "  )\n"
      ],
      "metadata": {
        "id": "x2O7vnbTaXqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vvX9Uzpx3rN",
        "outputId": "78862780-970c-415c-e427-c4c326f68b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Thought': 'Poll: Americans support quick diplomatic end to war in Ukraine'}"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "zGdW_nD9v6sH",
        "outputId": "019d2574-14cb-4ebf-9588-3428e7a6d6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n[INST]```Just a pic of my adorable cat```[/INST]\\npositive\\n\\n[INST]```Ideal chair seat height to kitchen counter ratio for doing dishes lol```[/INST]\\npositive\\n\\n[INST]```Today is going to be a great day!```[/INST]\\npositive\\n\\n[INST]```So I bought the repress of if you leave on the Spotify store thing anyone know when those go out?```[/INST]\\nnegative\\n\\n[INST]```Food sanitation company accused of employing at least 31 children on graveyard shifts in slaughterhouses:Packers Sanitation Services, Inc., or PSSI, a company contracted to work at slaughterhouses & meatpacking facilities.. allegedly employed at least 31 kids â€” one as young as 13```[/INST]\\nnegative\\n\\n[INST]```Poll: Americans support quick diplomatic end to war in Ukraine```[/INST]\\nnegative\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_rows = json.loads(\n",
        "    test.sample(100).to_json(orient='records')\n",
        ")"
      ],
      "metadata": {
        "id": "sekpX2yFl1wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making Predictions with the LLM**"
      ],
      "metadata": {
        "id": "EweHcelHmoMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize empty lists to store model predictions and ground truth values.\n",
        "model_predictions, ground_truths =[],[]"
      ],
      "metadata": {
        "id": "6rFevqkWmrQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Iterate through each row in the test data with a progress bar\n",
        "for row in tqdm(test_rows):\n",
        "  # Extract the input data (thought) from the current row, excluding the 'label'\n",
        "  test_input={i:row[i] for i in row if i!='Label'}\n",
        "  # Construct a few-shot prompt by combining system message, few-shot examples, and test input\n",
        "  few_shot_prompt = (\n",
        "        system_message + few_shot_examples +\n",
        "        prompt_template.format(\n",
        "            input_data=test_input['Thought'],\n",
        "            output=''\n",
        "        )\n",
        "    )\n",
        "  try:\n",
        "        # Use the model (lcpp_llm) to generate a response based on the few-shot prompt\n",
        "        response = lcpp_llm(\n",
        "            prompt=few_shot_prompt,\n",
        "            max_tokens=2,\n",
        "            temperature=0,\n",
        "            top_p=0.95,\n",
        "            repeat_penalty=1.2,\n",
        "            top_k=50,\n",
        "            stop=['INST'], # Dynamic stopping when such token is detected.\n",
        "            echo=False # do not return the prompt\n",
        "        )\n",
        "        # Extract the model's prediction from the response\n",
        "\n",
        "        prediction = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "\n",
        "        # Append the model's prediction to the 'model_predictions' list, lowercased and stripped of whitespace\n",
        "        model_predictions.append(prediction.strip().lower())\n",
        "\n",
        "        # Determine the ground truth label based on the row's 'label' value and append it to 'ground_truths'\n",
        "        if row['Label'] == 0:\n",
        "            ground_truths.append('negative')\n",
        "        else:\n",
        "            ground_truths.append('positive')\n",
        "  except ValueError as e:\n",
        "          # Handle any ValueErrors that may occur during the process and continue with the next row\n",
        "\n",
        "        print(e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9F5BKP4m1IG",
        "outputId": "984904b8-0b8f-4eea-e281-014cbd5a7856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1851.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     778.00 ms /   288 tokens (    2.70 ms per token,   370.18 tokens per second)\n",
            "llama_print_timings:        eval time =      60.22 ms /     1 runs   (   60.22 ms per token,    16.60 tokens per second)\n",
            "llama_print_timings:       total time =     845.91 ms /   289 tokens\n",
            "  1%|          | 1/100 [00:00<01:24,  1.17it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1877.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     628.52 ms /    24 tokens (   26.19 ms per token,    38.19 tokens per second)\n",
            "llama_print_timings:        eval time =      51.68 ms /     1 runs   (   51.68 ms per token,    19.35 tokens per second)\n",
            "llama_print_timings:       total time =     687.77 ms /    25 tokens\n",
            "  2%|▏         | 2/100 [00:01<01:14,  1.31it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1978.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     602.62 ms /    24 tokens (   25.11 ms per token,    39.83 tokens per second)\n",
            "llama_print_timings:        eval time =      50.91 ms /     1 runs   (   50.91 ms per token,    19.64 tokens per second)\n",
            "llama_print_timings:       total time =     660.13 ms /    25 tokens\n",
            "  3%|▎         | 3/100 [00:02<01:10,  1.38it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     690.07 ms /    27 tokens (   25.56 ms per token,    39.13 tokens per second)\n",
            "llama_print_timings:        eval time =      55.48 ms /     1 runs   (   55.48 ms per token,    18.02 tokens per second)\n",
            "llama_print_timings:       total time =     753.21 ms /    28 tokens\n",
            "  4%|▍         | 4/100 [00:02<01:10,  1.35it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1902.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     402.90 ms /    53 tokens (    7.60 ms per token,   131.55 tokens per second)\n",
            "llama_print_timings:        eval time =      63.44 ms /     1 runs   (   63.44 ms per token,    15.76 tokens per second)\n",
            "llama_print_timings:       total time =     473.65 ms /    54 tokens\n",
            "  5%|▌         | 5/100 [00:03<01:01,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     385.67 ms /    58 tokens (    6.65 ms per token,   150.39 tokens per second)\n",
            "llama_print_timings:        eval time =      71.57 ms /     1 runs   (   71.57 ms per token,    13.97 tokens per second)\n",
            "llama_print_timings:       total time =     464.29 ms /    59 tokens\n",
            "  6%|▌         | 6/100 [00:03<00:55,  1.70it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2010.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     698.94 ms /    26 tokens (   26.88 ms per token,    37.20 tokens per second)\n",
            "llama_print_timings:        eval time =      52.95 ms /     1 runs   (   52.95 ms per token,    18.88 tokens per second)\n",
            "llama_print_timings:       total time =     758.70 ms /    27 tokens\n",
            "  7%|▋         | 7/100 [00:04<01:00,  1.54it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1897.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     704.69 ms /    26 tokens (   27.10 ms per token,    36.90 tokens per second)\n",
            "llama_print_timings:        eval time =      51.34 ms /     1 runs   (   51.34 ms per token,    19.48 tokens per second)\n",
            "llama_print_timings:       total time =     763.85 ms /    27 tokens\n",
            "  8%|▊         | 8/100 [00:05<01:03,  1.45it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1765.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     496.71 ms /    17 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
            "llama_print_timings:        eval time =      52.35 ms /     1 runs   (   52.35 ms per token,    19.10 tokens per second)\n",
            "llama_print_timings:       total time =     556.45 ms /    18 tokens\n",
            "  9%|▉         | 9/100 [00:06<00:59,  1.53it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     700.08 ms /    26 tokens (   26.93 ms per token,    37.14 tokens per second)\n",
            "llama_print_timings:        eval time =      51.35 ms /     1 runs   (   51.35 ms per token,    19.48 tokens per second)\n",
            "llama_print_timings:       total time =     759.88 ms /    27 tokens\n",
            " 10%|█         | 10/100 [00:06<01:02,  1.45it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1906.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     701.03 ms /    25 tokens (   28.04 ms per token,    35.66 tokens per second)\n",
            "llama_print_timings:        eval time =      57.78 ms /     1 runs   (   57.78 ms per token,    17.31 tokens per second)\n",
            "llama_print_timings:       total time =     766.58 ms /    26 tokens\n",
            " 11%|█         | 11/100 [00:07<01:03,  1.39it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1874.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     399.86 ms /    44 tokens (    9.09 ms per token,   110.04 tokens per second)\n",
            "llama_print_timings:        eval time =      61.47 ms /     1 runs   (   61.47 ms per token,    16.27 tokens per second)\n",
            "llama_print_timings:       total time =     469.05 ms /    45 tokens\n",
            " 12%|█▏        | 12/100 [00:08<00:56,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1893.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     307.72 ms /    12 tokens (   25.64 ms per token,    39.00 tokens per second)\n",
            "llama_print_timings:        eval time =      53.10 ms /     1 runs   (   53.10 ms per token,    18.83 tokens per second)\n",
            "llama_print_timings:       total time =     368.28 ms /    13 tokens\n",
            " 13%|█▎        | 13/100 [00:08<00:49,  1.77it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1874.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     611.72 ms /    23 tokens (   26.60 ms per token,    37.60 tokens per second)\n",
            "llama_print_timings:        eval time =      60.95 ms /     1 runs   (   60.95 ms per token,    16.41 tokens per second)\n",
            "llama_print_timings:       total time =     680.06 ms /    24 tokens\n",
            " 14%|█▍        | 14/100 [00:09<00:51,  1.66it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1910.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     408.28 ms /    13 tokens (   31.41 ms per token,    31.84 tokens per second)\n",
            "llama_print_timings:        eval time =      53.38 ms /     1 runs   (   53.38 ms per token,    18.74 tokens per second)\n",
            "llama_print_timings:       total time =     469.35 ms /    14 tokens\n",
            " 15%|█▌        | 15/100 [00:09<00:48,  1.76it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1809.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     611.27 ms /    24 tokens (   25.47 ms per token,    39.26 tokens per second)\n",
            "llama_print_timings:        eval time =      57.74 ms /     1 runs   (   57.74 ms per token,    17.32 tokens per second)\n",
            "llama_print_timings:       total time =     676.32 ms /    25 tokens\n",
            " 16%|█▌        | 16/100 [00:10<00:50,  1.66it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     506.21 ms /    19 tokens (   26.64 ms per token,    37.53 tokens per second)\n",
            "llama_print_timings:        eval time =      54.58 ms /     1 runs   (   54.58 ms per token,    18.32 tokens per second)\n",
            "llama_print_timings:       total time =     568.58 ms /    20 tokens\n",
            " 17%|█▋        | 17/100 [00:10<00:49,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1897.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     414.60 ms /    13 tokens (   31.89 ms per token,    31.36 tokens per second)\n",
            "llama_print_timings:        eval time =      54.67 ms /     1 runs   (   54.67 ms per token,    18.29 tokens per second)\n",
            "llama_print_timings:       total time =     476.47 ms /    14 tokens\n",
            " 18%|█▊        | 18/100 [00:11<00:46,  1.77it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     705.81 ms /    25 tokens (   28.23 ms per token,    35.42 tokens per second)\n",
            "llama_print_timings:        eval time =      59.85 ms /     1 runs   (   59.85 ms per token,    16.71 tokens per second)\n",
            "llama_print_timings:       total time =     772.87 ms /    26 tokens\n",
            " 19%|█▉        | 19/100 [00:12<00:51,  1.59it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1960.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     819.51 ms /    29 tokens (   28.26 ms per token,    35.39 tokens per second)\n",
            "llama_print_timings:        eval time =      51.55 ms /     1 runs   (   51.55 ms per token,    19.40 tokens per second)\n",
            "llama_print_timings:       total time =     878.46 ms /    30 tokens\n",
            " 20%|██        | 20/100 [00:13<00:56,  1.41it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.43 ms /    55 tokens (    7.61 ms per token,   131.44 tokens per second)\n",
            "llama_print_timings:        eval time =      63.20 ms /     1 runs   (   63.20 ms per token,    15.82 tokens per second)\n",
            "llama_print_timings:       total time =     488.85 ms /    56 tokens\n",
            " 21%|██        | 21/100 [00:13<00:51,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     503.92 ms /    18 tokens (   28.00 ms per token,    35.72 tokens per second)\n",
            "llama_print_timings:        eval time =      58.28 ms /     1 runs   (   58.28 ms per token,    17.16 tokens per second)\n",
            "llama_print_timings:       total time =     569.12 ms /    19 tokens\n",
            " 22%|██▏       | 22/100 [00:14<00:48,  1.60it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1941.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     719.46 ms /    25 tokens (   28.78 ms per token,    34.75 tokens per second)\n",
            "llama_print_timings:        eval time =      55.41 ms /     1 runs   (   55.41 ms per token,    18.05 tokens per second)\n",
            "llama_print_timings:       total time =     781.82 ms /    26 tokens\n",
            " 23%|██▎       | 23/100 [00:14<00:52,  1.48it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1895.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     398.20 ms /    39 tokens (   10.21 ms per token,    97.94 tokens per second)\n",
            "llama_print_timings:        eval time =      60.52 ms /     1 runs   (   60.52 ms per token,    16.52 tokens per second)\n",
            "llama_print_timings:       total time =     465.94 ms /    40 tokens\n",
            " 24%|██▍       | 24/100 [00:15<00:46,  1.62it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1941.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     713.05 ms /    27 tokens (   26.41 ms per token,    37.87 tokens per second)\n",
            "llama_print_timings:        eval time =      56.66 ms /     1 runs   (   56.66 ms per token,    17.65 tokens per second)\n",
            "llama_print_timings:       total time =     777.14 ms /    28 tokens\n",
            " 25%|██▌       | 25/100 [00:16<00:50,  1.50it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1902.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     720.53 ms /    26 tokens (   27.71 ms per token,    36.08 tokens per second)\n",
            "llama_print_timings:        eval time =      56.57 ms /     1 runs   (   56.57 ms per token,    17.68 tokens per second)\n",
            "llama_print_timings:       total time =     784.52 ms /    27 tokens\n",
            " 26%|██▌       | 26/100 [00:17<00:52,  1.42it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1901.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     713.65 ms /    26 tokens (   27.45 ms per token,    36.43 tokens per second)\n",
            "llama_print_timings:        eval time =      56.04 ms /     1 runs   (   56.04 ms per token,    17.84 tokens per second)\n",
            "llama_print_timings:       total time =     776.51 ms /    27 tokens\n",
            " 27%|██▋       | 27/100 [00:17<00:53,  1.37it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1934.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     607.35 ms /    21 tokens (   28.92 ms per token,    34.58 tokens per second)\n",
            "llama_print_timings:        eval time =      57.50 ms /     1 runs   (   57.50 ms per token,    17.39 tokens per second)\n",
            "llama_print_timings:       total time =     673.04 ms /    22 tokens\n",
            " 28%|██▊       | 28/100 [00:18<00:51,  1.39it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1941.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     413.50 ms /    13 tokens (   31.81 ms per token,    31.44 tokens per second)\n",
            "llama_print_timings:        eval time =      54.00 ms /     1 runs   (   54.00 ms per token,    18.52 tokens per second)\n",
            "llama_print_timings:       total time =     475.59 ms /    14 tokens\n",
            " 29%|██▉       | 29/100 [00:18<00:46,  1.54it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1881.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     407.58 ms /    15 tokens (   27.17 ms per token,    36.80 tokens per second)\n",
            "llama_print_timings:        eval time =      56.77 ms /     1 runs   (   56.77 ms per token,    17.62 tokens per second)\n",
            "llama_print_timings:       total time =     472.04 ms /    16 tokens\n",
            " 30%|███       | 30/100 [00:19<00:41,  1.67it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1901.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     712.37 ms /    26 tokens (   27.40 ms per token,    36.50 tokens per second)\n",
            "llama_print_timings:        eval time =      53.70 ms /     1 runs   (   53.70 ms per token,    18.62 tokens per second)\n",
            "llama_print_timings:       total time =     773.93 ms /    27 tokens\n",
            " 31%|███       | 31/100 [00:20<00:45,  1.53it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1885.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     812.06 ms /    29 tokens (   28.00 ms per token,    35.71 tokens per second)\n",
            "llama_print_timings:        eval time =      56.85 ms /     1 runs   (   56.85 ms per token,    17.59 tokens per second)\n",
            "llama_print_timings:       total time =     876.47 ms /    30 tokens\n",
            " 32%|███▏      | 32/100 [00:21<00:49,  1.38it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1851.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     808.75 ms /    29 tokens (   27.89 ms per token,    35.86 tokens per second)\n",
            "llama_print_timings:        eval time =      55.60 ms /     1 runs   (   55.60 ms per token,    17.99 tokens per second)\n",
            "llama_print_timings:       total time =     871.82 ms /    30 tokens\n",
            " 33%|███▎      | 33/100 [00:22<00:51,  1.29it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     510.81 ms /    20 tokens (   25.54 ms per token,    39.15 tokens per second)\n",
            "llama_print_timings:        eval time =      55.95 ms /     1 runs   (   55.95 ms per token,    17.87 tokens per second)\n",
            "llama_print_timings:       total time =     574.68 ms /    21 tokens\n",
            " 34%|███▍      | 34/100 [00:22<00:47,  1.39it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1955.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     409.16 ms /    14 tokens (   29.23 ms per token,    34.22 tokens per second)\n",
            "llama_print_timings:        eval time =      55.45 ms /     1 runs   (   55.45 ms per token,    18.03 tokens per second)\n",
            "llama_print_timings:       total time =     472.39 ms /    15 tokens\n",
            " 35%|███▌      | 35/100 [00:23<00:42,  1.54it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1874.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     387.40 ms /    38 tokens (   10.19 ms per token,    98.09 tokens per second)\n",
            "llama_print_timings:        eval time =      66.40 ms /     1 runs   (   66.40 ms per token,    15.06 tokens per second)\n",
            "llama_print_timings:       total time =     461.56 ms /    39 tokens\n",
            " 36%|███▌      | 36/100 [00:23<00:38,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1890.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     697.16 ms /    25 tokens (   27.89 ms per token,    35.86 tokens per second)\n",
            "llama_print_timings:        eval time =      56.38 ms /     1 runs   (   56.38 ms per token,    17.74 tokens per second)\n",
            "llama_print_timings:       total time =     761.75 ms /    26 tokens\n",
            " 37%|███▋      | 37/100 [00:24<00:40,  1.54it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     706.09 ms /    25 tokens (   28.24 ms per token,    35.41 tokens per second)\n",
            "llama_print_timings:        eval time =      50.71 ms /     1 runs   (   50.71 ms per token,    19.72 tokens per second)\n",
            "llama_print_timings:       total time =     764.27 ms /    26 tokens\n",
            " 38%|███▊      | 38/100 [00:25<00:42,  1.46it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1829.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     379.30 ms /    33 tokens (   11.49 ms per token,    87.00 tokens per second)\n",
            "llama_print_timings:        eval time =      63.83 ms /     1 runs   (   63.83 ms per token,    15.67 tokens per second)\n",
            "llama_print_timings:       total time =     451.40 ms /    34 tokens\n",
            " 39%|███▉      | 39/100 [00:25<00:37,  1.61it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1834.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     403.50 ms /    14 tokens (   28.82 ms per token,    34.70 tokens per second)\n",
            "llama_print_timings:        eval time =      57.57 ms /     1 runs   (   57.57 ms per token,    17.37 tokens per second)\n",
            "llama_print_timings:       total time =     468.96 ms /    15 tokens\n",
            " 40%|████      | 40/100 [00:26<00:34,  1.73it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1960.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     501.19 ms /    18 tokens (   27.84 ms per token,    35.91 tokens per second)\n",
            "llama_print_timings:        eval time =      51.55 ms /     1 runs   (   51.55 ms per token,    19.40 tokens per second)\n",
            "llama_print_timings:       total time =     560.17 ms /    19 tokens\n",
            " 41%|████      | 41/100 [00:26<00:33,  1.74it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1899.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     597.02 ms /    22 tokens (   27.14 ms per token,    36.85 tokens per second)\n",
            "llama_print_timings:        eval time =      48.71 ms /     1 runs   (   48.71 ms per token,    20.53 tokens per second)\n",
            "llama_print_timings:       total time =     652.97 ms /    23 tokens\n",
            " 42%|████▏     | 42/100 [00:27<00:34,  1.66it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1860.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     406.76 ms /    13 tokens (   31.29 ms per token,    31.96 tokens per second)\n",
            "llama_print_timings:        eval time =      53.33 ms /     1 runs   (   53.33 ms per token,    18.75 tokens per second)\n",
            "llama_print_timings:       total time =     467.28 ms /    14 tokens\n",
            " 43%|████▎     | 43/100 [00:27<00:32,  1.77it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  2004.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     697.80 ms /    25 tokens (   27.91 ms per token,    35.83 tokens per second)\n",
            "llama_print_timings:        eval time =      54.82 ms /     1 runs   (   54.82 ms per token,    18.24 tokens per second)\n",
            "llama_print_timings:       total time =     759.82 ms /    26 tokens\n",
            " 44%|████▍     | 44/100 [00:28<00:35,  1.59it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1966.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     698.46 ms /    28 tokens (   24.95 ms per token,    40.09 tokens per second)\n",
            "llama_print_timings:        eval time =      52.36 ms /     1 runs   (   52.36 ms per token,    19.10 tokens per second)\n",
            "llama_print_timings:       total time =     757.97 ms /    29 tokens\n",
            " 45%|████▌     | 45/100 [00:29<00:36,  1.49it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1899.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     494.63 ms /    17 tokens (   29.10 ms per token,    34.37 tokens per second)\n",
            "llama_print_timings:        eval time =      50.81 ms /     1 runs   (   50.81 ms per token,    19.68 tokens per second)\n",
            "llama_print_timings:       total time =     553.00 ms /    18 tokens\n",
            " 46%|████▌     | 46/100 [00:29<00:34,  1.57it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1924.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     593.48 ms /    23 tokens (   25.80 ms per token,    38.75 tokens per second)\n",
            "llama_print_timings:        eval time =      54.61 ms /     1 runs   (   54.61 ms per token,    18.31 tokens per second)\n",
            "llama_print_timings:       total time =     656.30 ms /    24 tokens\n",
            " 47%|████▋     | 47/100 [00:30<00:34,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1850.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     400.91 ms /    13 tokens (   30.84 ms per token,    32.43 tokens per second)\n",
            "llama_print_timings:        eval time =      50.70 ms /     1 runs   (   50.70 ms per token,    19.72 tokens per second)\n",
            "llama_print_timings:       total time =     459.44 ms /    14 tokens\n",
            " 48%|████▊     | 48/100 [00:31<00:30,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1768.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     489.04 ms /    17 tokens (   28.77 ms per token,    34.76 tokens per second)\n",
            "llama_print_timings:        eval time =      50.33 ms /     1 runs   (   50.33 ms per token,    19.87 tokens per second)\n",
            "llama_print_timings:       total time =     547.18 ms /    18 tokens\n",
            " 49%|████▉     | 49/100 [00:31<00:29,  1.71it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1755.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     405.26 ms /    16 tokens (   25.33 ms per token,    39.48 tokens per second)\n",
            "llama_print_timings:        eval time =      54.66 ms /     1 runs   (   54.66 ms per token,    18.30 tokens per second)\n",
            "llama_print_timings:       total time =     467.71 ms /    17 tokens\n",
            " 50%|█████     | 50/100 [00:32<00:27,  1.81it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1953.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     688.47 ms /    25 tokens (   27.54 ms per token,    36.31 tokens per second)\n",
            "llama_print_timings:        eval time =      54.92 ms /     1 runs   (   54.92 ms per token,    18.21 tokens per second)\n",
            "llama_print_timings:       total time =     751.54 ms /    26 tokens\n",
            " 51%|█████     | 51/100 [00:32<00:30,  1.62it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1793.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     493.10 ms /    19 tokens (   25.95 ms per token,    38.53 tokens per second)\n",
            "llama_print_timings:        eval time =      53.52 ms /     1 runs   (   53.52 ms per token,    18.69 tokens per second)\n",
            "llama_print_timings:       total time =     554.65 ms /    20 tokens\n",
            " 52%|█████▏    | 52/100 [00:33<00:28,  1.66it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1862.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     591.50 ms /    21 tokens (   28.17 ms per token,    35.50 tokens per second)\n",
            "llama_print_timings:        eval time =      53.77 ms /     1 runs   (   53.77 ms per token,    18.60 tokens per second)\n",
            "llama_print_timings:       total time =     653.38 ms /    22 tokens\n",
            " 53%|█████▎    | 53/100 [00:34<00:29,  1.61it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.50 ms per token,  1984.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     784.31 ms /    31 tokens (   25.30 ms per token,    39.53 tokens per second)\n",
            "llama_print_timings:        eval time =      51.22 ms /     1 runs   (   51.22 ms per token,    19.52 tokens per second)\n",
            "llama_print_timings:       total time =     843.34 ms /    32 tokens\n",
            " 54%|█████▍    | 54/100 [00:34<00:31,  1.45it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.55 ms per token,  1806.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     783.37 ms /    31 tokens (   25.27 ms per token,    39.57 tokens per second)\n",
            "llama_print_timings:        eval time =      54.91 ms /     1 runs   (   54.91 ms per token,    18.21 tokens per second)\n",
            "llama_print_timings:       total time =     846.59 ms /    32 tokens\n",
            " 55%|█████▌    | 55/100 [00:35<00:33,  1.35it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1936.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     376.83 ms /    41 tokens (    9.19 ms per token,   108.80 tokens per second)\n",
            "llama_print_timings:        eval time =      58.09 ms /     1 runs   (   58.09 ms per token,    17.21 tokens per second)\n",
            "llama_print_timings:       total time =     442.71 ms /    42 tokens\n",
            " 56%|█████▌    | 56/100 [00:36<00:28,  1.53it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1881.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     583.53 ms /    21 tokens (   27.79 ms per token,    35.99 tokens per second)\n",
            "llama_print_timings:        eval time =      53.45 ms /     1 runs   (   53.45 ms per token,    18.71 tokens per second)\n",
            "llama_print_timings:       total time =     644.78 ms /    22 tokens\n",
            " 57%|█████▋    | 57/100 [00:36<00:28,  1.52it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1906.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     493.92 ms /    19 tokens (   26.00 ms per token,    38.47 tokens per second)\n",
            "llama_print_timings:        eval time =      53.65 ms /     1 runs   (   53.65 ms per token,    18.64 tokens per second)\n",
            "llama_print_timings:       total time =     555.27 ms /    20 tokens\n",
            " 58%|█████▊    | 58/100 [00:37<00:26,  1.59it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1879.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =     587.18 ms /    24 tokens (   24.47 ms per token,    40.87 tokens per second)\n",
            "llama_print_timings:        eval time =      56.20 ms /     1 runs   (   56.20 ms per token,    17.80 tokens per second)\n",
            "llama_print_timings:       total time =     651.20 ms /    25 tokens\n",
            " 59%|█████▉    | 59/100 [00:38<00:26,  1.56it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     585.58 ms /    23 tokens (   25.46 ms per token,    39.28 tokens per second)\n",
            "llama_print_timings:        eval time =      50.72 ms /     1 runs   (   50.72 ms per token,    19.72 tokens per second)\n",
            "llama_print_timings:       total time =     644.72 ms /    24 tokens\n",
            " 60%|██████    | 60/100 [00:38<00:25,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1813.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     588.58 ms /    22 tokens (   26.75 ms per token,    37.38 tokens per second)\n",
            "llama_print_timings:        eval time =      51.53 ms /     1 runs   (   51.53 ms per token,    19.41 tokens per second)\n",
            "llama_print_timings:       total time =     647.85 ms /    23 tokens\n",
            " 61%|██████    | 61/100 [00:39<00:25,  1.54it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1955.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.67 ms /    11 tokens (   27.06 ms per token,    36.95 tokens per second)\n",
            "llama_print_timings:        eval time =      49.16 ms /     1 runs   (   49.16 ms per token,    20.34 tokens per second)\n",
            "llama_print_timings:       total time =     354.08 ms /    12 tokens\n",
            " 62%|██████▏   | 62/100 [00:39<00:21,  1.77it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1947.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     586.81 ms /    24 tokens (   24.45 ms per token,    40.90 tokens per second)\n",
            "llama_print_timings:        eval time =      56.07 ms /     1 runs   (   56.07 ms per token,    17.83 tokens per second)\n",
            "llama_print_timings:       total time =     649.80 ms /    25 tokens\n",
            " 63%|██████▎   | 63/100 [00:40<00:21,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1936.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     379.22 ms /    52 tokens (    7.29 ms per token,   137.12 tokens per second)\n",
            "llama_print_timings:        eval time =      55.78 ms /     1 runs   (   55.78 ms per token,    17.93 tokens per second)\n",
            "llama_print_timings:       total time =     442.74 ms /    53 tokens\n",
            " 64%|██████▍   | 64/100 [00:40<00:19,  1.81it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1962.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     597.31 ms /    22 tokens (   27.15 ms per token,    36.83 tokens per second)\n",
            "llama_print_timings:        eval time =      50.32 ms /     1 runs   (   50.32 ms per token,    19.87 tokens per second)\n",
            "llama_print_timings:       total time =     654.77 ms /    23 tokens\n",
            " 65%|██████▌   | 65/100 [00:41<00:20,  1.71it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     401.67 ms /    15 tokens (   26.78 ms per token,    37.34 tokens per second)\n",
            "llama_print_timings:        eval time =      53.57 ms /     1 runs   (   53.57 ms per token,    18.67 tokens per second)\n",
            "llama_print_timings:       total time =     462.88 ms /    16 tokens\n",
            " 66%|██████▌   | 66/100 [00:42<00:18,  1.81it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1886.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     676.10 ms /    28 tokens (   24.15 ms per token,    41.41 tokens per second)\n",
            "llama_print_timings:        eval time =      53.41 ms /     1 runs   (   53.41 ms per token,    18.72 tokens per second)\n",
            "llama_print_timings:       total time =     737.20 ms /    29 tokens\n",
            " 67%|██████▋   | 67/100 [00:42<00:20,  1.63it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     773.25 ms /    30 tokens (   25.77 ms per token,    38.80 tokens per second)\n",
            "llama_print_timings:        eval time =      50.18 ms /     1 runs   (   50.18 ms per token,    19.93 tokens per second)\n",
            "llama_print_timings:       total time =     831.81 ms /    31 tokens\n",
            " 68%|██████▊   | 68/100 [00:43<00:21,  1.47it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1876.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     777.67 ms /    31 tokens (   25.09 ms per token,    39.86 tokens per second)\n",
            "llama_print_timings:        eval time =      51.57 ms /     1 runs   (   51.57 ms per token,    19.39 tokens per second)\n",
            "llama_print_timings:       total time =     836.78 ms /    32 tokens\n",
            " 69%|██████▉   | 69/100 [00:44<00:22,  1.37it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1863.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     369.08 ms /    40 tokens (    9.23 ms per token,   108.38 tokens per second)\n",
            "llama_print_timings:        eval time =      58.27 ms /     1 runs   (   58.27 ms per token,    17.16 tokens per second)\n",
            "llama_print_timings:       total time =     436.11 ms /    41 tokens\n",
            " 70%|███████   | 70/100 [00:44<00:19,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1848.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     579.46 ms /    21 tokens (   27.59 ms per token,    36.24 tokens per second)\n",
            "llama_print_timings:        eval time =      50.74 ms /     1 runs   (   50.74 ms per token,    19.71 tokens per second)\n",
            "llama_print_timings:       total time =     638.12 ms /    22 tokens\n",
            " 71%|███████   | 71/100 [00:45<00:18,  1.55it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1823.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     487.10 ms /    18 tokens (   27.06 ms per token,    36.95 tokens per second)\n",
            "llama_print_timings:        eval time =      49.13 ms /     1 runs   (   49.13 ms per token,    20.35 tokens per second)\n",
            "llama_print_timings:       total time =     544.80 ms /    19 tokens\n",
            " 72%|███████▏  | 72/100 [00:46<00:17,  1.61it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     379.02 ms /    64 tokens (    5.92 ms per token,   168.86 tokens per second)\n",
            "llama_print_timings:        eval time =      55.04 ms /     1 runs   (   55.04 ms per token,    18.17 tokens per second)\n",
            "llama_print_timings:       total time =     442.01 ms /    65 tokens\n",
            " 73%|███████▎  | 73/100 [00:46<00:15,  1.75it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1978.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     393.53 ms /    15 tokens (   26.24 ms per token,    38.12 tokens per second)\n",
            "llama_print_timings:        eval time =      54.48 ms /     1 runs   (   54.48 ms per token,    18.36 tokens per second)\n",
            "llama_print_timings:       total time =     455.58 ms /    16 tokens\n",
            " 74%|███████▍  | 74/100 [00:47<00:14,  1.85it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1839.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     584.85 ms /    24 tokens (   24.37 ms per token,    41.04 tokens per second)\n",
            "llama_print_timings:        eval time =      49.61 ms /     1 runs   (   49.61 ms per token,    20.16 tokens per second)\n",
            "llama_print_timings:       total time =     642.11 ms /    25 tokens\n",
            " 75%|███████▌  | 75/100 [00:47<00:14,  1.74it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     575.00 ms /    21 tokens (   27.38 ms per token,    36.52 tokens per second)\n",
            "llama_print_timings:        eval time =      48.70 ms /     1 runs   (   48.70 ms per token,    20.54 tokens per second)\n",
            "llama_print_timings:       total time =     631.41 ms /    22 tokens\n",
            " 76%|███████▌  | 76/100 [00:48<00:14,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1930.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     396.27 ms /    15 tokens (   26.42 ms per token,    37.85 tokens per second)\n",
            "llama_print_timings:        eval time =      48.81 ms /     1 runs   (   48.81 ms per token,    20.49 tokens per second)\n",
            "llama_print_timings:       total time =     452.60 ms /    16 tokens\n",
            " 77%|███████▋  | 77/100 [00:48<00:12,  1.80it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1968.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     481.39 ms /    17 tokens (   28.32 ms per token,    35.31 tokens per second)\n",
            "llama_print_timings:        eval time =      53.21 ms /     1 runs   (   53.21 ms per token,    18.79 tokens per second)\n",
            "llama_print_timings:       total time =     541.86 ms /    18 tokens\n",
            " 78%|███████▊  | 78/100 [00:49<00:12,  1.80it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1956.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     362.01 ms /    34 tokens (   10.65 ms per token,    93.92 tokens per second)\n",
            "llama_print_timings:        eval time =      58.50 ms /     1 runs   (   58.50 ms per token,    17.09 tokens per second)\n",
            "llama_print_timings:       total time =     427.79 ms /    35 tokens\n",
            " 79%|███████▉  | 79/100 [00:49<00:10,  1.92it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     296.06 ms /    12 tokens (   24.67 ms per token,    40.53 tokens per second)\n",
            "llama_print_timings:        eval time =      52.15 ms /     1 runs   (   52.15 ms per token,    19.18 tokens per second)\n",
            "llama_print_timings:       total time =     355.69 ms /    13 tokens\n",
            " 80%|████████  | 80/100 [00:50<00:09,  2.11it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  2008.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     481.65 ms /    19 tokens (   25.35 ms per token,    39.45 tokens per second)\n",
            "llama_print_timings:        eval time =      54.42 ms /     1 runs   (   54.42 ms per token,    18.38 tokens per second)\n",
            "llama_print_timings:       total time =     543.50 ms /    20 tokens\n",
            " 81%|████████  | 81/100 [00:50<00:09,  2.01it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  1994.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     396.26 ms /    16 tokens (   24.77 ms per token,    40.38 tokens per second)\n",
            "llama_print_timings:        eval time =      49.87 ms /     1 runs   (   49.87 ms per token,    20.05 tokens per second)\n",
            "llama_print_timings:       total time =     453.45 ms /    17 tokens\n",
            " 82%|████████▏ | 82/100 [00:51<00:08,  2.05it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     391.64 ms /    13 tokens (   30.13 ms per token,    33.19 tokens per second)\n",
            "llama_print_timings:        eval time =      52.98 ms /     1 runs   (   52.98 ms per token,    18.87 tokens per second)\n",
            "llama_print_timings:       total time =     452.59 ms /    14 tokens\n",
            " 83%|████████▎ | 83/100 [00:51<00:08,  2.08it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1888.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     485.09 ms /    18 tokens (   26.95 ms per token,    37.11 tokens per second)\n",
            "llama_print_timings:        eval time =      52.84 ms /     1 runs   (   52.84 ms per token,    18.93 tokens per second)\n",
            "llama_print_timings:       total time =     545.17 ms /    19 tokens\n",
            " 84%|████████▍ | 84/100 [00:52<00:08,  1.98it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2014.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     390.79 ms /    15 tokens (   26.05 ms per token,    38.38 tokens per second)\n",
            "llama_print_timings:        eval time =      53.32 ms /     1 runs   (   53.32 ms per token,    18.75 tokens per second)\n",
            "llama_print_timings:       total time =     451.73 ms /    16 tokens\n",
            " 85%|████████▌ | 85/100 [00:52<00:07,  2.03it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1968.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     768.70 ms /    29 tokens (   26.51 ms per token,    37.73 tokens per second)\n",
            "llama_print_timings:        eval time =      48.36 ms /     1 runs   (   48.36 ms per token,    20.68 tokens per second)\n",
            "llama_print_timings:       total time =     824.44 ms /    30 tokens\n",
            " 86%|████████▌ | 86/100 [00:53<00:08,  1.68it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.55 ms per token,  1805.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     672.25 ms /    25 tokens (   26.89 ms per token,    37.19 tokens per second)\n",
            "llama_print_timings:        eval time =      51.27 ms /     1 runs   (   51.27 ms per token,    19.50 tokens per second)\n",
            "llama_print_timings:       total time =     731.90 ms /    26 tokens\n",
            " 87%|████████▋ | 87/100 [00:54<00:08,  1.56it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1923.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     362.86 ms /    33 tokens (   11.00 ms per token,    90.94 tokens per second)\n",
            "llama_print_timings:        eval time =      53.93 ms /     1 runs   (   53.93 ms per token,    18.54 tokens per second)\n",
            "llama_print_timings:       total time =     424.73 ms /    34 tokens\n",
            " 88%|████████▊ | 88/100 [00:54<00:06,  1.73it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1926.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     764.45 ms /    29 tokens (   26.36 ms per token,    37.94 tokens per second)\n",
            "llama_print_timings:        eval time =      49.77 ms /     1 runs   (   49.77 ms per token,    20.09 tokens per second)\n",
            "llama_print_timings:       total time =     821.77 ms /    30 tokens\n",
            " 89%|████████▉ | 89/100 [00:55<00:07,  1.52it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1936.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     770.99 ms /    31 tokens (   24.87 ms per token,    40.21 tokens per second)\n",
            "llama_print_timings:        eval time =      53.44 ms /     1 runs   (   53.44 ms per token,    18.71 tokens per second)\n",
            "llama_print_timings:       total time =     832.19 ms /    32 tokens\n",
            " 90%|█████████ | 90/100 [00:56<00:07,  1.40it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1976.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     363.53 ms /    33 tokens (   11.02 ms per token,    90.78 tokens per second)\n",
            "llama_print_timings:        eval time =      52.17 ms /     1 runs   (   52.17 ms per token,    19.17 tokens per second)\n",
            "llama_print_timings:       total time =     423.61 ms /    34 tokens\n",
            " 91%|█████████ | 91/100 [00:56<00:05,  1.59it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1876.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.80 ms /    25 tokens (   26.87 ms per token,    37.21 tokens per second)\n",
            "llama_print_timings:        eval time =      48.56 ms /     1 runs   (   48.56 ms per token,    20.59 tokens per second)\n",
            "llama_print_timings:       total time =     728.15 ms /    26 tokens\n",
            " 92%|█████████▏| 92/100 [00:57<00:05,  1.51it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1813.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.45 ms /    10 tokens (   29.54 ms per token,    33.85 tokens per second)\n",
            "llama_print_timings:        eval time =      47.59 ms /     1 runs   (   47.59 ms per token,    21.01 tokens per second)\n",
            "llama_print_timings:       total time =     350.48 ms /    11 tokens\n",
            " 93%|█████████▎| 93/100 [00:57<00:04,  1.75it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1913.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     362.92 ms /    40 tokens (    9.07 ms per token,   110.22 tokens per second)\n",
            "llama_print_timings:        eval time =      56.12 ms /     1 runs   (   56.12 ms per token,    17.82 tokens per second)\n",
            "llama_print_timings:       total time =     426.94 ms /    41 tokens\n",
            " 94%|█████████▍| 94/100 [00:58<00:03,  1.88it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  2000.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     684.27 ms /    27 tokens (   25.34 ms per token,    39.46 tokens per second)\n",
            "llama_print_timings:        eval time =      48.87 ms /     1 runs   (   48.87 ms per token,    20.46 tokens per second)\n",
            "llama_print_timings:       total time =     740.25 ms /    28 tokens\n",
            " 95%|█████████▌| 95/100 [00:59<00:02,  1.67it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2012.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     482.80 ms /    17 tokens (   28.40 ms per token,    35.21 tokens per second)\n",
            "llama_print_timings:        eval time =      48.58 ms /     1 runs   (   48.58 ms per token,    20.59 tokens per second)\n",
            "llama_print_timings:       total time =     538.63 ms /    18 tokens\n",
            " 96%|█████████▌| 96/100 [00:59<00:02,  1.71it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1930.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     389.86 ms /    13 tokens (   29.99 ms per token,    33.35 tokens per second)\n",
            "llama_print_timings:        eval time =      50.87 ms /     1 runs   (   50.87 ms per token,    19.66 tokens per second)\n",
            "llama_print_timings:       total time =     448.24 ms /    14 tokens\n",
            " 97%|█████████▋| 97/100 [01:00<00:01,  1.83it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1731.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.51 ms /    11 tokens (   26.86 ms per token,    37.22 tokens per second)\n",
            "llama_print_timings:        eval time =      48.34 ms /     1 runs   (   48.34 ms per token,    20.69 tokens per second)\n",
            "llama_print_timings:       total time =     351.79 ms /    12 tokens\n",
            " 98%|█████████▊| 98/100 [01:00<00:00,  2.03it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.77 ms /     9 tokens (   32.53 ms per token,    30.74 tokens per second)\n",
            "llama_print_timings:        eval time =      48.38 ms /     1 runs   (   48.38 ms per token,    20.67 tokens per second)\n",
            "llama_print_timings:       total time =     348.75 ms /    10 tokens\n",
            " 99%|█████████▉| 99/100 [01:00<00:00,  2.21it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     829.22 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     577.22 ms /    22 tokens (   26.24 ms per token,    38.11 tokens per second)\n",
            "llama_print_timings:        eval time =      50.76 ms /     1 runs   (   50.76 ms per token,    19.70 tokens per second)\n",
            "llama_print_timings:       total time =     635.33 ms /    23 tokens\n",
            "100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "PE9I-IPFwIuC",
        "outputId": "80daf2f0-0bf6-4ee0-b42e-b10cf5662a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>Classify the human thinking based on the input into the following label\\n-negative\\n-positive\\n\\nThought will be delimited by triple backticks in the input.\\nAnswer only 'positive' or 'negative'.\\n\\nInstructions:\\n1. Classified the input only as: 'positive', 'negative'.\\n\\nYour answer should strictly contain the label: 'positive' or 'negative'.\\nThe output should be in JSON format like the following\\n{'Label': <positive, negative>}\\n\\n\\nSome examples of expected output are provided below as guidance.<</SYS>>[/INST]\\n\\n[INST]```Just a pic of my adorable cat```[/INST]\\npositive\\n\\n[INST]```Ideal chair seat height to kitchen counter ratio for doing dishes lol```[/INST]\\npositive\\n\\n[INST]```Today is going to be a great day!```[/INST]\\npositive\\n\\n[INST]```So I bought the repress of if you leave on the Spotify store thing anyone know when those go out?```[/INST]\\nnegative\\n\\n[INST]```Food sanitation company accused of employing at least 31 children on graveyard shifts in slaughterhouses:Packers Sanitation Services, Inc., or PSSI, a company contracted to work at slaughterhouses & meatpacking facilities.. allegedly employed at least 31 kids â€” one as young as 13```[/INST]\\nnegative\\n\\n[INST]```Poll: Americans support quick diplomatic end to war in Ukraine```[/INST]\\nnegative\\n\\n[INST]```The play's conclusion was dull and left me dissatisfied.```[/INST]\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(model_predictions) #Counter is a Python library that is often used to count the occurrences of elements in an iterable, such as a list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGYkcxLZm2gR",
        "outputId": "848f2575-e9c4-434b-8dbe-9eaaa5e529df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'positive': 66, 'negative': 34})"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(ground_truths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T216WfQs0TA",
        "outputId": "cdbd1790-a70b-4c7f-b724-56dffffea263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'positive': 63, 'negative': 37})"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accuracy on the Test Set**"
      ],
      "metadata": {
        "id": "T1OctOMns3J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truths = np.array(ground_truths)\n",
        "model_predictions = np.array(model_predictions)\n",
        "\n",
        "\n",
        "#ground_truths contains the true labels (ground truth) for a set of examples.\n",
        "#model_predictions contains the labels predicted by a machine learning model for the same set of examples."
      ],
      "metadata": {
        "id": "Ns0L5DY_s5cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zucb-barutGt",
        "outputId": "535ca73d-fcbe-4223-9933-e3e84473cea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'negative', 'negative', 'positive',\n",
              "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
              "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
              "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
              "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
              "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
              "       'positive', 'negative', 'positive', 'negative', 'positive',\n",
              "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
              "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
              "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
              "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
              "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
              "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'negative', 'negative', 'negative'],\n",
              "      dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ehJW0Rux7T",
        "outputId": "765c60ac-4024-4f95-e5f0-84633cb6f1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
              "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
              "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'positive', 'negative', 'negative',\n",
              "       'positive', 'negative', 'negative', 'positive', 'negative',\n",
              "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
              "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
              "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
              "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
              "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
              "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
              "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
              "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
              "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
              "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
              "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "       'negative', 'positive', 'negative', 'negative', 'negative'],\n",
              "      dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ground_truths == model_predictions).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWkSoNK1tMoJ",
        "outputId": "ddaec222-b506-4e0b-8e76-149150a4d04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP = ((model_predictions == 'positive') & (ground_truths == 'positive')).sum()\n",
        "FP = ((model_predictions == 'positive') & (ground_truths == 'negative')).sum()\n",
        "FN = ((model_predictions == 'negative') & (ground_truths == 'positive')).sum()\n",
        "TN = ((model_predictions == 'negative') & (ground_truths == 'negative')).sum()\n",
        "precision = TP / (TP+FP)"
      ],
      "metadata": {
        "id": "vfgpq8aMtTi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezzgoV95tcZd",
        "outputId": "3f5a104a-aa00-47f6-c348-028fd7e634aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8636363636363636"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall= TP / (TP+FN)"
      ],
      "metadata": {
        "id": "uiPSL4XE-chB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kV5K7qHF5S5",
        "outputId": "2ea83484-f6b4-4657-d699-12ca0078b23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9047619047619048"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F1_score=2*((precision*recall)/(precision+recall))"
      ],
      "metadata": {
        "id": "jYDtxzHHF78r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yncyhXLuGLkW",
        "outputId": "5cbe5159-0f4c-4366-8200-156d487d5b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8837209302325582"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy=(TN+TP)/(TN+FP+TP+FN)"
      ],
      "metadata": {
        "id": "AUOkndmqGNGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6OzY6zQGZSl",
        "outputId": "46f1a2d1-b5d6-4849-81de-5e4f7985ba87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "ac = metrics.accuracy_score(ground_truths,model_predictions)"
      ],
      "metadata": {
        "id": "hnQD2TIeGdpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG-qvSgrHsvl",
        "outputId": "a343ef51-cfc3-40f6-f760-00cf6d39e46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf_mat = metrics.confusion_matrix(ground_truths,model_predictions)"
      ],
      "metadata": {
        "id": "ngK9AhzNICgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_mat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYMMQ2vR25o",
        "outputId": "b9578ee7-f7a3-49d2-b46d-cdab4676c933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[28,  9],\n",
              "       [ 6, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SCfUTAqqOacW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cf_mat, cmap=, display_labels=['Negative','Positive'])\n",
        "#cm_display.plot()\n",
        "\n",
        "sns.heatmap(cf_mat,annot = True,fmt ='g', cmap='Blues', xticklabels=['positive','negative'],\n",
        "            yticklabels=['positive','negative'])\n",
        "\n",
        "plt.title(\"Confusion Matrix of Llama-2 13B model\")\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "UKm5PHOaUpJX",
        "outputId": "459d33cd-6b6a-4bc3-d97b-ed468f101d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "k4NW5R4dWRfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " print(classification_report(ground_truths,model_predictions,target_names=['Positive','Negative']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bph-SClhWCdE",
        "outputId": "afe9cd5d-fdbd-4beb-dced-86a26ce4efb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.82      0.76      0.79        37\n",
            "    Negative       0.86      0.90      0.88        63\n",
            "\n",
            "    accuracy                           0.85       100\n",
            "   macro avg       0.84      0.83      0.84       100\n",
            "weighted avg       0.85      0.85      0.85       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-cC8VlgSl4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}